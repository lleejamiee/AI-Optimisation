{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "Q4O1r8jrjLr5"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Iteration 4.0\n",
    "Introduces strategies to address issues encountered in previous iterations with large documents. These strategies include:\n",
    "1. Iterative retrieval\n",
    "2. Context pairing\n",
    "3. Context cleaning\n"
   ],
   "metadata": {
    "id": "KWs8H7DfJnaB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "af6i2P1vuU-U"
   },
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# HuggingFace API Key in Google Colab Secrets\n",
    "\n",
    "LLAMA_CLOUD_API_KEY = userdata.get('LLAMA_CLOUD_API_KEY')\n",
    "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "\n",
    "os.environ['LLAMA_CLOUD_API_KEY'] = LLAMA_CLOUD_API_KEY\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HP_f-yxtxH1m"
   },
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser, LangchainNodeParser\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings\n",
    "LlamaIndex global configurations."
   ],
   "metadata": {
    "id": "AXKiqd4vROOs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ix8FDOZLxYmy"
   },
   "source": [
    "Settings.llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=os.getenv(\"GROQ_API_KEY\"), temperature=0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "Preprocessing documents using LlamaParse to extract text from files.\\\n",
    "https://docs.cloud.llamaindex.ai/llamaparse/features/python_usage"
   ],
   "metadata": {
    "id": "Er5ehk1X_Stc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "outdated_docs = LlamaParse(num_workers=8, split_by_page=0,  result_type=\"text\").load_data(\"demo_guide.docx\") # Set 'split_by_page=0' to remove splitting by default\n",
    "reference_docs = LlamaParse(split_by_page=0, result_type=\"text\").load_data(\"demo_reference.docx\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YeAt7OeLuaVT",
    "outputId": "a2eb6f20-8d82-459f-b16e-267616922374"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chunking"
   ],
   "metadata": {
    "id": "87NYElyLguMe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from haystack import Document\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "\n",
    "# Splitter\n",
    "outdated_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=512)\n",
    "reference_splitter = DocumentSplitter(split_by=\"passage\", split_length=1, split_overlap=0)\n",
    "\n",
    "# Get Chunks\n",
    "outdated_chunks = outdated_splitter.get_nodes_from_documents(outdated_docs)\n",
    "reference_chunks = reference_splitter.run(documents=[Document(content=reference_docs[0].text)])"
   ],
   "metadata": {
    "id": "sR4RyJTXUCC3"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, document in enumerate(reference_chunks['documents']):\n",
    "  print(f\"Chunk #{i+1}\\n{document.content}\")"
   ],
   "metadata": {
    "id": "tYefq83uvDYz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core.schema import TextNode"
   ],
   "metadata": {
    "id": "Xk2St2STsBlo"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nodes = []\n",
    "for chunk in outdated_chunks:\n",
    "  nodes.append(TextNode(text=chunk.get_content()))\n",
    "\n",
    "outdated_index = VectorStoreIndex(nodes)"
   ],
   "metadata": {
    "id": "n7fp2zxHsEOo"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# demo: display text chunks\n",
    "from IPython.display import Markdown\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "  display(Markdown(f\"# ðŸ”´Chunk: {i+1}\"))\n",
    "  print(node.get_content())"
   ],
   "metadata": {
    "id": "FwTJHx8pqsCq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieval & Context Pairing\n",
    "A context pair consists of the reference material text used for retrieval and the corresponding retrieved chunk."
   ],
   "metadata": {
    "id": "2h0l4hO7hbZ5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# Create a retriever over the outdated material index\n",
    "retriever = VectorIndexRetriever(index=outdated_index, similarity_top_k=6)\n",
    "context_pairs = []\n",
    "\n",
    "# Iterate through each section of the reference material\n",
    "for reference_material in reference_chunks['documents']:\n",
    "  # Search and retrieve from the index using a single section(update) of the reference material\n",
    "  query_bundle = QueryBundle(reference_material.content)\n",
    "  retrieved = retriever.retrieve(query_bundle)\n",
    "\n",
    "  # Pair the retrieved chunk with the reference material\n",
    "  reference, outdated = reference_material.content, retrieved[0].text\n",
    "  context_pairs.append(tuple([reference, outdated]))\n"
   ],
   "metadata": {
    "id": "t9WiTR5Wu6NK"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# demo: display the context pairs\n",
    "for reference, outdated in context_pairs:\n",
    "  display(Markdown(f\"## ðŸ”´Context Pair\"))\n",
    "\n",
    "  display(Markdown(\"**Reference Material:**\"))\n",
    "  print(reference)\n",
    "\n",
    "  display(Markdown(\"**Outdated Material:**\"))\n",
    "  print(outdated)"
   ],
   "metadata": {
    "id": "Ut3e1x7gpID9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Text for Replacement"
   ],
   "metadata": {
    "id": "APjascIj-31N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Method 1: Chunk Cleaning\n",
    "Clean the chunk by removing irrelevant text. The cleaned text will serve as a marker for string replacement."
   ],
   "metadata": {
    "id": "JJ7QF2LTi-m1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# System prompt\n",
    "sys_prompt=\"\"\"\n",
    "You are tasked with identifying and retaining only the sections of the document that need to be replaced based on the provided reference material. Your goal is to extract the content that is directly relevant for replacement without adding or modifying the existing text.\n",
    "\n",
    "Instructions:\n",
    "1. Review both the provided document chunk and reference material to determine which sections are directly relevant for replacement.\n",
    "2. Extract the content that will be replaced, including any references or context that should be retained as part of the replacement.\n",
    "3. Ensure that the extracted section remains as close as possible in length and context to the original content that will be replaced.\n",
    "4. Do not include any additional content or modifications outside of what is necessary for the update.\n",
    "5. Do not include any explanations, introductions, or additional text.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "9LrUeVgVh8Yi"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompte template\n",
    "prompt = f\"\"\"\n",
    "Please extract and retain only the section of the document that is directly relevant to the content being updated. Here is the specific section to keep:\n",
    "\n",
    "Here is a chunk from a report that needs refinement:\n",
    "{context_pairs[0][1]}\n",
    "\n",
    "Here is the reference material to use for determining relevance:\n",
    "{context_pairs[0][0]}\n",
    "\n",
    "Ensure that only this section is extracted and preserved for the update.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "O0zHoQ2NhAhH"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# demo: uncomment to see the final prompt\n",
    "# print(prompt)"
   ],
   "metadata": {
    "id": "z3A4THyplNv5"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}],\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ],
   "metadata": {
    "id": "CO9kHzUf7dxh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Method 2: Update the outdated text chunk and use it as the replacement text\n",
    "This method will not work when there are context pairs that share the same outdated text chunk.\n",
    "\n",
    "When working with multiple context pairs that reference the same outdated text chunk, the method breaks down because once the first update is applied, the original text is altered, making subsequent updates impossible if they rely on the same outdated text.\n"
   ],
   "metadata": {
    "id": "Q4O1r8jrjLr5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sys_prompt = \"\"\"\n",
    "Your task is to selectively update the provided document section using the given reference material.\n",
    "\n",
    "Instructions:\n",
    "1. Read the document section and reference material carefully.\n",
    "2. Identify specific information in the document section that is directly addressed by the reference material.\n",
    "3. Update ONLY the information that is explicitly contradicted or updated by the reference material.\n",
    "4. Keep all text from the document that is still relevant and not specifically addressed by the reference material.\n",
    "5. Maintain the original style, format, and structure of the section.\n",
    "6. Do not add any new information that is not explicitly provided in the reference material.\n",
    "7. Provide ONLY the updated section text.\n",
    "8. Do not include any explanations, introductions, or additional text.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "ZhofBkNAknCI"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = f\"\"\"\n",
    "Here is some context for information to update a section from a chunk that was chunked from a report:\n",
    "{context_pairs[0][1]}\n",
    "\n",
    "Here is the reference material:\n",
    "{context_pairs[0][0]}\n",
    "\n",
    "Update the section of the chunk with the context, remove all other sections of the chunk that have not been updated.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "9L2PeycRi7IS"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# demo: uncomment to see the final prompt\n",
    "# print(prompt)"
   ],
   "metadata": {
    "id": "LNCO6NkeB-LZ"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}],\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ],
   "metadata": {
    "id": "wEUjGMCo_ps3"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
